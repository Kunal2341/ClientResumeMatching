{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Understanding of Resumes\n",
    "- Code basicially first converts the PDF to a Image\n",
    "- Then using google cloud vision API, it converts that image to text\n",
    "    - Using my personal JSON Code\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (__init__.py, line 3)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"C:\\Users\\kunal\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\"\u001b[0m, line \u001b[0;32m3326\u001b[0m, in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-13b1d3dca18c>\"\u001b[1;36m, line \u001b[1;32m1\u001b[1;36m, in \u001b[1;35m<module>\u001b[1;36m\u001b[0m\n\u001b[1;33m    from firebase import firebase\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\kunal\\Anaconda3\\lib\\site-packages\\firebase\\__init__.py\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    from .async import process_pool\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from firebase import firebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_leaf(path):\n",
    "    head, tail = ntpath.split(path)\n",
    "    return tail or ntpath.basename(head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Document_402.jpg'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_leaf(pathString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathString = '/Users/kunal/Documents/ResumeNLPVdart/Testing_Delete/Document_402.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n",
      "U\n",
      "s\n",
      "e\n",
      "r\n",
      "s\n",
      "/\n",
      "k\n",
      "u\n",
      "n\n",
      "a\n",
      "l\n",
      "/\n",
      "D\n",
      "o\n",
      "c\n",
      "u\n",
      "m\n",
      "e\n",
      "n\n",
      "t\n",
      "s\n",
      "/\n",
      "R\n",
      "e\n",
      "s\n",
      "u\n",
      "m\n",
      "e\n",
      "N\n",
      "L\n",
      "P\n",
      "V\n",
      "d\n",
      "a\n",
      "r\n",
      "t\n",
      "/\n",
      "T\n",
      "e\n",
      "s\n",
      "t\n",
      "i\n",
      "n\n",
      "g\n",
      "_\n",
      "D\n",
      "e\n",
      "l\n",
      "e\n",
      "t\n",
      "e\n",
      "/\n",
      "D\n",
      "o\n",
      "c\n",
      "u\n",
      "m\n",
      "e\n",
      "n\n",
      "t\n",
      "_\n",
      "4\n",
      "0\n",
      "2\n",
      ".\n",
      "j\n",
      "p\n",
      "g\n"
     ]
    }
   ],
   "source": [
    "for i in pathString:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert PDF to Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pdf_2_image(uploaded_image_path, uploaded_image):\n",
    "    project_dir = os.getcwd()\n",
    "    os.chdir(uploaded_image_path)\n",
    "    file_name = str(uploaded_image).replace('.pdf','')\n",
    "    output_file = file_name+'.jpg'\n",
    "    pages = convert_from_path(uploaded_image, 200,poppler_path='/Users/kunal/Documents/VdartWorking/Poppler/poppler-0.68.0_x86/poppler-0.68.0/bin/')\n",
    "    for page in pages:\n",
    "        page.save(output_file, 'JPEG')\n",
    "        break\n",
    "    #os.chdir(project_dir)\n",
    "    #img = Image.open(output_file)\n",
    "    #img = img.resize(img_size, PIL.Image.ANTIALIAS)\n",
    "    #img.save(output_file)\n",
    "    return output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Document_402.jpg'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_pdf_2_image('/Users/kunal/Documents/ResumeNLPVdart/Testing_Delete/', \"Document_402.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Image to Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyDIR = \"/Users/kunal/Documents/VdartWorking/GOOGLEAPI/vdartrealfakevision-0f30bdc03946.json\"\n",
    "credentials = service_account.Credentials.from_service_account_file(keyDIR)\n",
    "client = vision.ImageAnnotatorClient(credentials=credentials)\n",
    "with io.open('/Users/kunal/Documents/ResumeNLPVdart/Testing_Delete/Document_402.jpg', 'rb') as image_file:\n",
    "    content = image_file.read()\n",
    "image = vision.types.Image(content=content)\n",
    "response = client.text_detection(image=image)\n",
    "texts = response.text_annotations\n",
    "totalString = ''\n",
    "for text in texts:\n",
    "    totalString+=text.description\n",
    "totalString = totalString.rsplit(' ', 1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"NISARGA HASSAN SREEDHAR\\nSan Jose, California |+1 (925) 789-8911| nisarga.nishu20@gmail.com | www.linkedin.com/in/nisarga-sreedhar-39938516b\\nEDUCATION:\\nMaster's in Electrical Engineering (Computer Networking), San Jose State University, California, USA.\\nCoursework: Internetworking, Broadband communications, Network Security, Internet of Things (IoT), Voice over IP\\nBachelor of Engineering in Telecommunication Engineering, Dayananda Sagar College of Engineering, Visvesvaraya\\nTechnological University, India\\nMay 2020\\nJune 2017\\nTECHNICAL SKILLS:\\nNetwork technologies: HTTP, DNS, DHCP, HTTPS, TLS-SSL, TCP/IP, UDP, IPV4, IPV6, ICMP, OSPF, BGP, ARP, VLAN, STP,\\nSIP, IPS, IDS, NAT, IS-IS, 802.11, MPLS, WPA2, WPA3, Packet level troubleshooting\\nProgramming: Python\\nOS Platform: Linux (Ubuntu, CentOS), Kali Linux, Cisco IOS\\nTools and IDE: Advanced Design System (ADS), Wireshark, VMware Workstation, VirtualBox, GNS3, Cisco Packet Tracer, PUTTY\\nCERTIFICATION:\\nCisco Certified Network Associate (CCNA) 200-301\\nAWS Certified Cloud Practitioner (CLF-C01)\\n(In Progress)\\n(In progress)\\nEXPERIENCE:\\nJune 2019 - July 2019\\nMarmon Food & Beverages Technologies, Cornelius, India\\nNetwork Engineer Intern\\nPython based Serial Communication (IoT)\\nUsed an Iot Dongle to read a file, convert it into a packet by adding header and footer and transmit serially.\\nPython code was written to send the file from dongle to Food Holding Bin.\\nACADEMIC PROJECTS:\\nSecure routing in IoT networks\\nAug 2019 - current\\nDesign and configure an IoT based network using Cisco Packet Tracer.\\nPerform a Man in the Middle attack to one of the devices using Kali Linux.\\nDetection of the attack and solution to the problem faced.\\nIllumino: IoT Smart Light\\nAug 2019 - Dec 2019\\nCreate a hardware of an IoT smart light using Arduino ESP8266 and Cayenne IoT Platform.\\nDesigned to operate in three modes: Auto mode, Lamp mode, Security mode.\\nUse of Cayenne web application to detect temperature and provides a siren at thresholds.\\nVoice over IP for Wireless Ad Hoc Networks (WANET)\\nAug 2019 - Dec 2019\\nSimple Call Establishment between two clients in a WANET that have registered with the Asterisk server.\\nCall on Hold with one user client to attend another client.\\nCall Conferencing between all three clients, all performed using X-Lite softphone software.\\nExperiencing Virtualization using Virtual Box\\nJan 2019 - April 2019\\nWorked on Open vSwitch in Virtual Box on an Ubuntu machine to run ovs and its versions successfully.\\nDemonstrated how the VLANS are implemented, three VMs and one virtual switch is created.\\nAttempted to communicate between the VMs and observed the PING result.\\nCorporate Company Network Design\\nAug 2018 - Dec 2018\\n• Designed and implemented a basic corporate network topology for the interconnection between offices with\\nswitches, routers, and hosts.\\n• Implemented the design using routing protocols such as OSPF, BGP, DNS, VLAN, STP, IP, DHCP and HSRP.\\nTested and troubleshot configurations in the console to check the communication between the networks.\\nDesign of X-Band 8PSK Modulator using ADS\\nJan 2017 - April 2017\\nDesigned various components of the modulator used in a satellite at ISRO (Indian Space Research Organization), Bangalore.\\nPerformed optimization of the components at 8.75GHZ frequency using the tools available in ADS to obtain the desired results of\\nInsertion loss, Return loss and Isolation\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_image_to_text():\n",
    "    keyDIR = \"/Users/kunal/Documents/VdartWorking/GOOGLEAPI/vdartrealfakevision-0f30bdc03946.json\"\n",
    "    credentials = service_account.Credentials.from_service_account_file(keyDIR)\n",
    "    client = vision.ImageAnnotatorClient(credentials=credentials)\n",
    "\n",
    "    with io.open(MAINIMAGEFILEPNG, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "    image = vision.types.Image(content=content)\n",
    "    response = client.document_text_detection(image=image)\n",
    "    textDocument = []\n",
    "    blockConfid = []\n",
    "    paraConfid = []\n",
    "    wordConfid = []\n",
    "    for page in response.full_text_annotation.pages:\n",
    "        for block in page.blocks:\n",
    "            for paragraph in block.paragraphs:\n",
    "                for word in paragraph.words:\n",
    "                    word_text = ''.join([symbol.text for symbol in word.symbols])\n",
    "                    textDocument.append(word_text)\n",
    "                    blockConfid.append(block.confidence)\n",
    "                    paraConfid.append(paragraph.confidence)\n",
    "                    wordConfid.append(word.confidence)\n",
    "\n",
    "    if response.error.message:\n",
    "        raise Exception(\n",
    "            '{}\\nFor more info on error messages, check: '\n",
    "            'https://cloud.google.com/apis/design/errors'.format(\n",
    "                response.error.message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic NLP Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"NISARGA HASSAN SREEDHAR\n",
    "San Jose, California | +1 (925) 789-8911| nisarga.nishu20@gmail.com | www.linkedin.com/in/nisarga-sreedhar-39938516b\n",
    "EDUCATION:\n",
    "Master’s in Electrical Engineering (Computer Networking), San Jose State University, California, USA. May 2020\n",
    "Coursework: Internetworking, Broadband communications, Network Security, Internet of Things (IoT), Voice over IP\n",
    "Bachelor of Engineering in Telecommunication Engineering, Dayananda Sagar College of Engineering, Visvesvaraya\n",
    "Technological University, India June 2017\n",
    "TECHNICAL SKILLS:\n",
    "Network technologies: HTTP, DNS, DHCP, HTTPS, TLS-SSL, TCP/IP, UDP, IPv4, IPv6, ICMP, OSPF, BGP, ARP, VLAN, STP,\n",
    "SIP, IPS, IDS, NAT, IS-IS, 802.11, MPLS, WPA2, WPA3, Packet level troubleshooting\n",
    "Programming: Python\n",
    "OS Platform: Linux (Ubuntu, CentOS), Kali Linux, Cisco IOS\n",
    "Tools and IDE: Advanced Design System (ADS), Wireshark, VMware Workstation, VirtualBox, GNS3, Cisco Packet Tracer, PuTTY\n",
    "CERTIFICATION:\n",
    "• Cisco Certified Network Associate (CCNA) 200-301 (In Progress)\n",
    "• AWS Certified Cloud Practitioner (CLF-C01) (In progress)\n",
    "EXPERIENCE:\n",
    "Marmon Food & Beverages Technologies, Cornelius, India June 2019 - July 2019\n",
    "Network Engineer Intern\n",
    "• Python based Serial Communication (IoT)\n",
    "• Used an Iot Dongle to read a file, convert it into a packet by adding header and footer and transmit serially.\n",
    "• Python code was written to send the file from dongle to Food Holding Bin.\n",
    "ACADEMIC PROJECTS:\n",
    "Secure routing in IoT networks Aug 2019 - current\n",
    "• Design and configure an IoT based network using Cisco Packet Tracer.\n",
    "• Perform a Man in the Middle attack to one of the devices using Kali Linux.\n",
    "• Detection of the attack and solution to the problem faced.\n",
    "Illumino: IoT Smart Light Aug 2019 - Dec 2019\n",
    "• Create a hardware of an IoT smart light using Arduino ESP8266 and Cayenne IoT Platform.\n",
    "• Designed to operate in three modes: Auto mode, Lamp mode, Security mode.\n",
    "• Use of Cayenne web application to detect temperature and provides a siren at thresholds.\n",
    "Voice over IP for Wireless Ad Hoc Networks (WANET) Aug 2019 - Dec 2019\n",
    "• Simple Call Establishment between two clients in a WANET that have registered with the Asterisk server.\n",
    "• Call on Hold with one user client to attend another client.\n",
    "• Call Conferencing between all three clients, all performed using X-Lite softphone software.\n",
    "Experiencing Virtualization using Virtual Box Jan 2019 - April 2019\n",
    "• Worked on Open vSwitch in Virtual Box on an Ubuntu machine to run ovs and its versions successfully.\n",
    "• Demonstrated how the VLANs are implemented, three VMs and one virtual switch is created.\n",
    "• Attempted to communicate between the VMs and observed the PING result.\n",
    "Corporate Company Network Design Aug 2018 - Dec 2018\n",
    "• Designed and implemented a basic corporate network topology for the interconnection between offices with\n",
    "switches, routers, and hosts.\n",
    "• Implemented the design using routing protocols such as OSPF, BGP, DNS, VLAN, STP, IP, DHCP and HSRP.\n",
    "• Tested and troubleshot configurations in the console to check the communication between the networks.\n",
    "Design of X-Band 8PSK Modulator using ADS Jan 2017 - April 2017\n",
    "• Designed various components of the modulator used in a satellite at ISRO (Indian Space Research Organization), Bangalore.\n",
    "• Performed optimization of the components at 8.75GHz frequency using the tools available in ADS to obtain the desired results of\n",
    "Insertion loss, Return loss and Isolation loss.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [t for t in text.split()]\n",
    "\n",
    "freq = nltk.FreqDist(tokens)\n",
    "\n",
    "for key,val in freq.items():\n",
    "\n",
    "    print (str(key) + ':' + str(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq.plot(20, cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tokens = tokens[:]\n",
    "sr = stopwords.words('english')\n",
    "for token in tokens:\n",
    "    if token in stopwords.words('english'):\n",
    "        clean_tokens.remove(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq.plot(20,cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem('working'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('increases'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gension nltk spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "preshed.maps does not export expected C function map_clear",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1c080c493f24>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"en_core_web_sm\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Apple is looking at buying U.K. startup for $1 billion\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mthinc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneural\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprefer_gpu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequire_gpu\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcli\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcli_info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mglossary\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexplain\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\spacy\\pipeline\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0municode_literals\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpipes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTagger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDependencyParser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEntityRecognizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEntityLinker\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpipes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTextCategorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTensorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPipe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSentencizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmorphologizer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMorphologizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpipes.pyx\u001b[0m in \u001b[0;36minit spacy.pipeline.pipes\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\spacy\\pipeline\\functions.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlanguage\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcomponent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatcher\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMatcher\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfilter_spans\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\spacy\\matcher\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmatcher\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMatcher\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mphrasematcher\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPhraseMatcher\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdependencymatcher\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDependencyMatcher\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mphrasematcher.pyx\u001b[0m in \u001b[0;36minit spacy.matcher.phrasematcher\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: preshed.maps does not export expected C function map_clear"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'document' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-477a366b7086>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdocument\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'document' is not defined"
     ]
    }
   ],
   "source": [
    "document = ' '.join([i for i in document.split() if i not in stop])\n",
    "sentences = nltk.sent_tokenize(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [nltk.word_tokenize(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\kunal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-5dd0614b2d2f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sentences' is not defined"
     ]
    }
   ],
   "source": [
    "sentences = [nltk.pos_tag(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "string = \"\"\"\n",
    "NISARGA HASSAN SREEDHAR\n",
    "San Jose, California | +1 (925) 789-8911| nisarga.nishu20@gmail.com | www.linkedin.com/in/nisarga-sreedhar-39938516b\n",
    "\"\"\"\n",
    "\n",
    "def extract_phone_numbers(string):\n",
    "    r = re.compile(r'(\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4})')\n",
    "    phone_numbers = r.findall(string)\n",
    "    return [re.sub(r'\\D', '', number) for number in phone_numbers]\n",
    "\n",
    "def extract_email_addresses(string):\n",
    "    r = re.compile(r'[\\w\\.-]+@[\\w\\.-]+')\n",
    "    return r.findall(string)\n",
    "\n",
    "def ie_preprocess(document):\n",
    "    document = ' '.join([i for i in document.split() if i not in stop])\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "    return sentences\n",
    "\n",
    "def extract_names(document):\n",
    "    names = []\n",
    "    sentences = ie_preprocess(document)\n",
    "    for tagged_sentence in sentences:\n",
    "        for chunk in nltk.ne_chunk(tagged_sentence):\n",
    "            if type(chunk) == nltk.tree.Tree:\n",
    "                if chunk.label() == 'PERSON':\n",
    "                    names.append(' '.join([c[0] for c in chunk]))\n",
    "    return names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\kunal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\kunal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = extract_phone_numbers(string)\n",
    "emails = extract_email_addresses(string)\n",
    "names = extract_names(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['9257898911', '3993851']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nisarga.nishu20@gmail.com']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['San Jose']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "349.091px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

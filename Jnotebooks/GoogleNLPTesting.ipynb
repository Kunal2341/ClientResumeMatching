{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Process Creating Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports the Google Cloud client library\n",
    "from google.cloud import language\n",
    "from google.cloud.language import enums\n",
    "from google.cloud.language import types\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import language_v1\n",
    "from google.cloud.language_v1 import enums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyDIR = \"/Users/kunal/Documents/ResumeNLPVdart/APIKEYSGOOGLE/resumeMatcher-NLP_create_data.json\"\n",
    "credentials = service_account.Credentials.from_service_account_file(keyDIR)\n",
    "client = language.LanguageServiceClient(credentials=credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_content' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-2b2da912018d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtype_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menums\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDocument\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mType\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPLAIN_TEXT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlanguage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"en\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdocument\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"content\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtext_content\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"type\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtype_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"language\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mencoding_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menums\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEncodingType\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUTF8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text_content' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_analyze_entities(text_content):\n",
    "    keyDIR = \"/Users/kunal/Documents/ResumeNLPVdart/APIKEYSGOOGLE/resumeMatcher-NLP_create_data.json\"\n",
    "    credentials = service_account.Credentials.from_service_account_file(keyDIR)\n",
    "    client = language_v1.LanguageServiceClient(credentials=credentials)\n",
    "    type_ = enums.Document.Type.PLAIN_TEXT\n",
    "    language = \"en\"\n",
    "    document = {\"content\": text_content, \"type\": type_, \"language\": language}\n",
    "    encoding_type = enums.EncodingType.UTF8\n",
    "    \n",
    "    response = client.analyze_entities(document, encoding_type=encoding_type)\n",
    "\n",
    "    # Loop through entitites returned from the API\n",
    "    for entity in response.entities:\n",
    "        print(u\"Representative name for the entity: {}\".format(entity.name))\n",
    "        # Get entity type, e.g. PERSON, LOCATION, ADDRESS, NUMBER, et al\n",
    "        print(u\"Entity type: {}\".format(enums.Entity.Type(entity.type).name))\n",
    "        # Get the salience score associated with the entity in the [0, 1.0] range\n",
    "        print(u\"Salience score: {}\".format(entity.salience))\n",
    "        # Loop over the metadata associated with entity. For many known entities,\n",
    "        # the metadata is a Wikipedia URL (wikipedia_url) and Knowledge Graph MID (mid).\n",
    "        # Some entity types may have additional metadata, e.g. ADDRESS entities\n",
    "        # may have metadata for the address street_name, postal_code, et al.\n",
    "        for metadata_name, metadata_value in entity.metadata.items():\n",
    "            print(u\"{}: {}\".format(metadata_name, metadata_value))\n",
    "\n",
    "        # Loop over the mentions of this entity in the input document.\n",
    "        # The API currently supports proper noun mentions.\n",
    "        for mention in entity.mentions:\n",
    "            print(u\"Mention text: {}\".format(mention.text.content))\n",
    "\n",
    "            # Get the mention type, e.g. PROPER for proper noun\n",
    "            print(\n",
    "                u\"Mention type: {}\".format(enums.EntityMention.Type(mention.type).name)\n",
    "            )\n",
    "        print(\"\")\n",
    "\n",
    "    # Get the language of the text, which will be the same as\n",
    "    # the language specified in the request or, if not specified,\n",
    "    # the automatically-detected language.\n",
    "    print(u\"Language of the text: {}\".format(response.language))\n",
    "def sample_analyze_sentiment(text_content):\n",
    "    keyDIR = \"/Users/kunal/Documents/ResumeNLPVdart/APIKEYSGOOGLE/resumeMatcher-NLP_create_data.json\"\n",
    "    credentials = service_account.Credentials.from_service_account_file(keyDIR)\n",
    "    client = language_v1.LanguageServiceClient(credentials=credentials)\n",
    "    type_ = enums.Document.Type.PLAIN_TEXT\n",
    "    language = \"en\"\n",
    "    document = {\"content\": text_content, \"type\": type_, \"language\": language}\n",
    "    encoding_type = enums.EncodingType.UTF8\n",
    "\n",
    "    response = client.analyze_sentiment(document, encoding_type=encoding_type)\n",
    "    # Get overall sentiment of the input document\n",
    "    print(u\"Document sentiment score: {}\".format(response.document_sentiment.score))\n",
    "    print(\n",
    "        u\"Document sentiment magnitude: {}\".format(\n",
    "            response.document_sentiment.magnitude\n",
    "        )\n",
    "    )\n",
    "    # Get sentiment for all sentences in the document\n",
    "    for sentence in response.sentences:\n",
    "        print(u\"Sentence text: {}\".format(sentence.text.content))\n",
    "        print(u\"Sentence sentiment score: {}\".format(sentence.sentiment.score))\n",
    "        print(u\"Sentence sentiment magnitude: {}\".format(sentence.sentiment.magnitude))\n",
    "\n",
    "    # Get the language of the text, which will be the same as\n",
    "    # the language specified in the request or, if not specified,\n",
    "    # the automatically-detected language.\n",
    "    print(u\"Language of the text: {}\".format(response.language))\n",
    "def sample_analyze_syntax(text_content):\n",
    "    keyDIR = \"/Users/kunal/Documents/ResumeNLPVdart/APIKEYSGOOGLE/resumeMatcher-NLP_create_data.json\"\n",
    "    credentials = service_account.Credentials.from_service_account_file(keyDIR)\n",
    "    client = language_v1.LanguageServiceClient(credentials=credentials)\n",
    "    type_ = enums.Document.Type.PLAIN_TEXT\n",
    "    language = \"en\"\n",
    "    document = {\"content\": text_content, \"type\": type_, \"language\": language}\n",
    "    encoding_type = enums.EncodingType.UTF8\n",
    "    \n",
    "    response = client.analyze_syntax(document, encoding_type=encoding_type)\n",
    "    # Loop through tokens returned from the API\n",
    "    for token in response.tokens:\n",
    "        # Get the text content of this token. Usually a word or punctuation.\n",
    "        text = token.text\n",
    "        print(u\"Token text: {}\".format(text.content))\n",
    "        print(\n",
    "            u\"Location of this token in overall document: {}\".format(text.begin_offset)\n",
    "        )\n",
    "        # Get the part of speech information for this token.\n",
    "        # Parts of spech are as defined in:\n",
    "        # http://www.lrec-conf.org/proceedings/lrec2012/pdf/274_Paper.pdf\n",
    "        part_of_speech = token.part_of_speech\n",
    "        # Get the tag, e.g. NOUN, ADJ for Adjective, et al.\n",
    "        print(\n",
    "            u\"Part of Speech tag: {}\".format(\n",
    "                enums.PartOfSpeech.Tag(part_of_speech.tag).name\n",
    "            )\n",
    "        )\n",
    "        # Get the voice, e.g. ACTIVE or PASSIVE\n",
    "        print(u\"Voice: {}\".format(enums.PartOfSpeech.Voice(part_of_speech.voice).name))\n",
    "        # Get the tense, e.g. PAST, FUTURE, PRESENT, et al.\n",
    "        print(u\"Tense: {}\".format(enums.PartOfSpeech.Tense(part_of_speech.tense).name))\n",
    "        # See API reference for additional Part of Speech information available\n",
    "        # Get the lemma of the token. Wikipedia lemma description\n",
    "        # https://en.wikipedia.org/wiki/Lemma_(morphology)\n",
    "        print(u\"Lemma: {}\".format(token.lemma))\n",
    "        # Get the dependency tree parse information for this token.\n",
    "        # For more information on dependency labels:\n",
    "        # http://www.aclweb.org/anthology/P13-2017\n",
    "        dependency_edge = token.dependency_edge\n",
    "        print(u\"Head token index: {}\".format(dependency_edge.head_token_index))\n",
    "        print(\n",
    "            u\"Label: {}\".format(enums.DependencyEdge.Label(dependency_edge.label).name)\n",
    "        )\n",
    "\n",
    "    # Get the language of the text, which will be the same as\n",
    "    # the language specified in the request or, if not specified,\n",
    "    # the automatically-detected language.\n",
    "    print(u\"Language of the text: {}\".format(response.language))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document sentiment score: 0.699999988079071\n",
      "Document sentiment magnitude: 0.699999988079071\n",
      "Sentence text: Proficient in optimal UX solutions\n",
      "Sentence sentiment score: 0.699999988079071\n",
      "Sentence sentiment magnitude: 0.699999988079071\n",
      "Language of the text: en\n"
     ]
    }
   ],
   "source": [
    "sample_analyze_entities(\"\")\n",
    "sample_analyze_sentiment(\"Proficient in optimal UX solutions\")\n",
    "sample_analyze_syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceList = ['Oleg Kotliarsky',\n",
    " '(720) 987-8054',\n",
    " 'olegkot@gmail.com',\n",
    " 'Profile highlights:',\n",
    " 'Web Development Engineer',\n",
    " 'Vast experience designing, programming and leading enterprise web applications development',\n",
    " 'Proficient in optimal UX solutions',\n",
    " 'Great experience in developing reusable components to optimize development time and maintenance',\n",
    " 'Ability to provide technical leadership and clear guidance to development team',\n",
    " 'High skills to research, evaluate and implement right technical solution for the enterprise application',\n",
    " 'Technical Knowledge:',\n",
    " 'JavaScript, TypeScript, ReactJS, Action Script 3.0, Java, PHP',\n",
    " 'Angular, AngularJS, RXJS, Karma, Java Spring, İBATIS, Apache Struts',\n",
    " 'Oracle, MongoDB, SQL Server, MYSQL',\n",
    " 'angular-cli, npm, bower, gulp, GIT',\n",
    " 'Languages:',\n",
    " 'Frameworks:',\n",
    " 'Databases:',\n",
    " 'Tools:',\n",
    " 'Professional Experience: Aug. 2017 - April 2020 Senior Web Developer, Comcast, CO',\n",
    " 'Designed and developed new app features (Columbo - ESL). (Angular6/Angular UI, Remedy, JAVA, Oracle DB):',\n",
    " 'data driven \"case create wizard\" with back-end precheck and dynamic restructure of the steps',\n",
    " '- case resolve \"stepper\" with variable number of corresponding relative issues',\n",
    " '- reusable components - \"keyword\" search, attachments list, \"add attachments\", PDF viewer, util service with multitude of helper functions.',\n",
    " '- HTTP request wrappers, HTTP response interceptor for unified error handling',\n",
    " 'Developed a web app (Columbo - Executive Support Line). (Hybrid AngularJS/Angular UI, Remedy, JAVA, Oracle DB); Maintain and support GIT Hub of the project',\n",
    " 'Dec. 2013 – June 2017 Senior UI Developer / Team Lead / Scrum Master, DN2K, CO',\n",
    " 'Developed customers, search, navigation modules for \"MyDairyCentral\" web app portal, sensors tiles carousel, responsive design, etc. (Angular4, angular-cli, Jasmine/Karma)',\n",
    " 'Developed sensors tiles carousel, set karma unit tests framework for \"MyGrowCentral\" web app portal (AngularJS, Node, responsive design, JHipster, Jasmine/Karma)',\n",
    " 'Developed different features of the \"MyAGCentral\" web app portal, including navigation tree, work orders flow, etc. (AngularJS, Node, Mongo) - client Sagelnsights: https://www.sageinsights.com/',\n",
    " 'Transitioned from Backbone to Angular framework (team effort) of the \"MyAGCentral\" web app portal',\n",
    " 'October 2011 - Dec. 2013 Web Developer Expert, Amdocs Inc., CO',\n",
    " 'Developed \"Order Entry\" web application for entering order to the Amdocs Enterprise order management system (HTML5/CSS3, JavaScript/jQuery, AJAX/DWR, JSP/Java 7, Oracle, Java Spring, iBatis)',\n",
    " 'Developed e-signature web app using previously developed JavaScript/jQuery/JSP/Java6/Spring2 framework for sending client\\'s contract to \"on-the-fly\" signature creation (integration with docusign.com service).',\n",
    " 'Developed part of the real time payment integration flow utilizing Amdocs EAI framework called JESI. (Java, JSP, SOAP, Oracle, JavaScript, jQuery)',\n",
    " 'Developed Flex \"Executive Advisor\" report tool for serving different types of client statistics presented in a rich graphic interactive way (Flex 4.6, Blaze DS, Java, Oracle, Action Script 3)',\n",
    " 'October 2010 – Oct. 2011 Web Developer Contractor, Rose International (for Amdocs Inc.), CO',\n",
    " 'Developed iLink Mobile app (running on iPad Safari – used home developed framework) for sales representatives [client: DexOne]. (JavaScript, jQuery, jQTouch, AJAX, HTML5/webkit, Java, Spring, iBatis, Oracle, JSP);',\n",
    " 'Developed an iPad web application framework for rapid development of iOS apps that look like',\n",
    " 'March 2009 - October 2010 GSET Engineer, Wall Street on Demand (now Markit on Demand), Boulder, CO',\n",
    " 'Developed Entitlements management intranet tool [client: Goldman Sachs]. (Java, Struts, Sybase, JSP, JavaScript, jQuery, AJAX);',\n",
    " 'March 2008 – March 2009 Team Lead Developer, Wall Street on Demand (now Markit on Demand), Boulder, CO',\n",
    " 'Leaded team of web developers, working on line of Stocks Research Websites [client: Schwab Institutional] (ASP, JavaScript, AJAX);',\n",
    " 'August 2005 – March 2008 Senior Web Developer, Wall Street on Demand (now Markit on Demand), Boulder, CO',\n",
    " 'Developed Web site architecture and determine software requirements.',\n",
    " 'Created and optimized content for the Web site, including planning, design, integration and testing of Web-site related code.',\n",
    " 'Planned and designed new featured web sites according to client requirements. Close interaction with graphic designers, project managers and QA members of our group;',\n",
    " 'Developed Real Time DB driven web sites with Stocks market content, including price quotes graphics, charts, news, alerts etc. Schwab group projects (ASP, JScript, JavaScript, AJAX);',\n",
    " 'April 2004 – August 2005 Freelancer Web Developer, Denver, CO',\n",
    " 'Developed full code circle from templates to launch (PHP/MySql, JavaScript, HTML, CSS);',\n",
    " 'Created Graphics: logos, bullets, complete design (Photoshop, Flash MX);',\n",
    " 'Promoted web sites in search engines positioning (1st page positions in Google, Yahoo on several key words).',\n",
    " 'April 2003 – April 2004',\n",
    " 'Web developer, Gteko (purchased by Microsoft), Raanana, Israel',\n",
    " '- Developed JavaScript/VBScript active client side (ActiveX event\\'s handling flow: version checking, upgrading, downloading and installation) of different \"e-support\" accounts (HP, Canon, AOL, Dell, NEC, Lenovo, etc)',\n",
    " 'Developed JavaScript classes reflecting graphic presentation of ActiveX control downloading and installation processes;',\n",
    " 'Developed a full JavaScript based interface for ActiveX control data processing - JavaScript/DOM/DHTML based model for PC scanned data show. Was a leading developer for GTWebCheck product part called \"Upgrade Advisor\" or \"Summary Report\".',\n",
    " 'June 2002 – April 2003 Freelancer Web developer, Tel-Aviv , Israel',\n",
    " 'Complete web sites production (Programming development, PHP/MYSQL/JavaScript/HTML/CSS index + forum (OOP);',\n",
    " \"Created graphic design according to Client's requirements (logo, bullets, layout – Photoshop, Flash); Made domain name registration; Assisted in identity development, marketing, online promotion and launch;\",\n",
    " 'Maintained web mastering; Promoted web sites for search engines positioning August 1999 - May 2002',\n",
    " 'Web Developer, Snapshield Ltd, Tel Aviv, Israel',\n",
    " \"Created Web design (Photoshop, Flash) for Web based application for remote data management for tens of thousands of clients of the leading Snapshield's Telecom Encryption Service (SNAP);\",\n",
    " 'Programmed part of the SNAP application (Customer Care) using ASP, JavaScript, VBScript, CSS, IIS 4;',\n",
    " 'Set required configuration for SSL on MS IIS4.',\n",
    " 'Created web based client-side application \"Snapshield\\'s Security Algorithm Benchmark\" for dynamic online calculating and show for different TI DSP platforms and Snapshield algorithms. (ASP, CSS, DHTML)',\n",
    " 'Created Flash Animated Company Business and Technical Presentations (online, cds) in Macromedia',\n",
    " 'Flash 5; Integrated video streaming to companies web site (JavaScript, DHTML)',\n",
    " 'Created graphic and technical design, developed, published and web mastered three generations of the\\ncompany\\'s web site. (HTML/DHTML, JavaScript, CSS, Macromedia Flash 5, Adobe Photoshop 5.5;',\n",
    " 'Assisted in new branding process (migrating from Microlink to Snapshield)',\n",
    " 'Education:',\n",
    " 'BS and MS in Mathematics and Mechanics',\n",
    " 'St. Petersburg State University',\n",
    " 'Courses:',\n",
    " '\"Oracle Certified Associate\", iTerra Consulting, Ic., Denver, USA.',\n",
    " '\"Design for Multimedia\", ORT Syngalovsky College, Tel Aviv. (800 hours)',\n",
    " '\"JavaScript - DHTML – DOM\", Sela group, Tel Aviv (30 hours)',\n",
    " '\"OOD/OOP for C++ and Java\", \"Network TCP/IP\", Tel-Ran, Rishon-Lezion (1000 hours)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Imports the Google Cloud client library\n",
    "from google.cloud import language\n",
    "from google.cloud.language import enums\n",
    "from google.cloud.language import types\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import language_v1\n",
    "from google.cloud.language_v1 import enums\n",
    "\n",
    "def sample_analyze_entities(text_content):\n",
    "    keyDIR = \"/Users/kunal/Documents/ResumeNLPVdart/APIKEYSGOOGLE/resumeMatcher-NLP_create_data.json\"\n",
    "    credentials = service_account.Credentials.from_service_account_file(keyDIR)\n",
    "    client = language_v1.LanguageServiceClient(credentials=credentials)\n",
    "    type_ = enums.Document.Type.PLAIN_TEXT\n",
    "    language = \"en\"\n",
    "    document = {\"content\": text_content, \"type\": type_, \"language\": language}\n",
    "    encoding_type = enums.EncodingType.UTF8\n",
    "\n",
    "    response = client.analyze_entities(document, encoding_type=encoding_type)\n",
    "\n",
    "    # Loop through entitites returned from the API\n",
    "    for entity in response.entities:\n",
    "        print(u\"Representative name for the entity: {}\".format(entity.name))\n",
    "        # Get entity type, e.g. PERSON, LOCATION, ADDRESS, NUMBER, et al\n",
    "        print(u\"Entity type: {}\".format(enums.Entity.Type(entity.type).name))\n",
    "        # Get the salience score associated with the entity in the [0, 1.0] range\n",
    "        print(u\"Salience score: {}\".format(entity.salience))\n",
    "        # Loop over the metadata associated with entity. For many known entities,\n",
    "        # the metadata is a Wikipedia URL (wikipedia_url) and Knowledge Graph MID (mid).\n",
    "        # Some entity types may have additional metadata, e.g. ADDRESS entities\n",
    "        # may have metadata for the address street_name, postal_code, et al.\n",
    "        for metadata_name, metadata_value in entity.metadata.items():\n",
    "            print(u\"{}: {}\".format(metadata_name, metadata_value))\n",
    "\n",
    "        # Loop over the mentions of this entity in the input document.\n",
    "        # The API currently supports proper noun mentions.\n",
    "        for mention in entity.mentions:\n",
    "            print(u\"Mention text: {}\".format(mention.text.content))\n",
    "\n",
    "            # Get the mention type, e.g. PROPER for proper noun\n",
    "            print(\n",
    "                u\"Mention type: {}\".format(enums.EntityMention.Type(mention.type).name)\n",
    "            )\n",
    "        print(\"\")\n",
    "\n",
    "    # Get the language of the text, which will be the same as\n",
    "    # the language specified in the request or, if not specified,\n",
    "    # the automatically-detected language.\n",
    "    print(u\"Language of the text: {}\".format(response.language))\n",
    "\n",
    "def sample_analyze_syntax(text_content):\n",
    "    keyDIR = \"/Users/kunal/Documents/ResumeNLPVdart/APIKEYSGOOGLE/resumeMatcher-NLP_create_data.json\"\n",
    "    credentials = service_account.Credentials.from_service_account_file(keyDIR)\n",
    "    client = language_v1.LanguageServiceClient(credentials=credentials)\n",
    "    type_ = enums.Document.Type.PLAIN_TEXT\n",
    "    language = \"en\"\n",
    "    document = {\"content\": text_content, \"type\": type_, \"language\": language}\n",
    "    encoding_type = enums.EncodingType.UTF8\n",
    "\n",
    "    response = client.analyze_syntax(document, encoding_type=encoding_type)\n",
    "    # Loop through tokens returned from the API\n",
    "    for token in response.tokens:\n",
    "        # Get the text content of this token. Usually a word or punctuation.\n",
    "        text = token.text\n",
    "        print(u\"Token text: {}\".format(text.content))\n",
    "        print(\n",
    "            u\"Location of this token in overall document: {}\".format(text.begin_offset)\n",
    "        )\n",
    "        # Get the part of speech information for this token.\n",
    "        # Parts of spech are as defined in:\n",
    "        # http://www.lrec-conf.org/proceedings/lrec2012/pdf/274_Paper.pdf\n",
    "        part_of_speech = token.part_of_speech\n",
    "        # Get the tag, e.g. NOUN, ADJ for Adjective, et al.\n",
    "        print(\n",
    "            u\"Part of Speech tag: {}\".format(\n",
    "                enums.PartOfSpeech.Tag(part_of_speech.tag).name\n",
    "            )\n",
    "        )\n",
    "        # Get the voice, e.g. ACTIVE or PASSIVE\n",
    "        print(u\"Voice: {}\".format(enums.PartOfSpeech.Voice(part_of_speech.voice).name))\n",
    "        # Get the tense, e.g. PAST, FUTURE, PRESENT, et al.\n",
    "        print(u\"Tense: {}\".format(enums.PartOfSpeech.Tense(part_of_speech.tense).name))\n",
    "        # See API reference for additional Part of Speech information available\n",
    "        # Get the lemma of the token. Wikipedia lemma description\n",
    "        # https://en.wikipedia.org/wiki/Lemma_(morphology)\n",
    "        print(u\"Lemma: {}\".format(token.lemma))\n",
    "        # Get the dependency tree parse information for this token.\n",
    "        # For more information on dependency labels:\n",
    "        # http://www.aclweb.org/anthology/P13-2017\n",
    "        dependency_edge = token.dependency_edge\n",
    "        print(u\"Head token index: {}\".format(dependency_edge.head_token_index))\n",
    "        print(\n",
    "            u\"Label: {}\".format(enums.DependencyEdge.Label(dependency_edge.label).name)\n",
    "        )\n",
    "\n",
    "    # Get the language of the text, which will be the same as\n",
    "    # the language specified in the request or, if not specified,\n",
    "    # the automatically-detected language.\n",
    "    print(u\"Language of the text: {}\".format(response.language))\n",
    "\n",
    "def sample_analyze_entity_sentiment(text_content):\n",
    "    keyDIR = \"/Users/kunal/Documents/ResumeNLPVdart/APIKEYSGOOGLE/resumeMatcher-NLP_create_data.json\"\n",
    "    credentials = service_account.Credentials.from_service_account_file(keyDIR)\n",
    "    client = language_v1.LanguageServiceClient(credentials=credentials)\n",
    "    type_ = enums.Document.Type.PLAIN_TEXT\n",
    "    language = \"en\"\n",
    "    document = {\"content\": text_content, \"type\": type_, \"language\": language}\n",
    "    encoding_type = enums.EncodingType.UTF8\n",
    "\n",
    "    response = client.analyze_entity_sentiment(document, encoding_type=encoding_type)\n",
    "    # Loop through entitites returned from the API\n",
    "    for entity in response.entities:\n",
    "        print(u\"Representative name for the entity: {}\".format(entity.name))\n",
    "        # Get entity type, e.g. PERSON, LOCATION, ADDRESS, NUMBER, et al\n",
    "        print(u\"Entity type: {}\".format(enums.Entity.Type(entity.type).name))\n",
    "        # Get the salience score associated with the entity in the [0, 1.0] range\n",
    "        print(u\"Salience score: {}\".format(entity.salience))\n",
    "        # Get the aggregate sentiment expressed for this entity in the provided document.\n",
    "        sentiment = entity.sentiment\n",
    "        print(u\"Entity sentiment score: {}\".format(sentiment.score))\n",
    "        print(u\"Entity sentiment magnitude: {}\".format(sentiment.magnitude))\n",
    "        # Loop over the metadata associated with entity. For many known entities,\n",
    "        # the metadata is a Wikipedia URL (wikipedia_url) and Knowledge Graph MID (mid).\n",
    "        # Some entity types may have additional metadata, e.g. ADDRESS entities\n",
    "        # may have metadata for the address street_name, postal_code, et al.\n",
    "        for metadata_name, metadata_value in entity.metadata.items():\n",
    "            print(u\"{} = {}\".format(metadata_name, metadata_value))\n",
    "\n",
    "        # Loop over the mentions of this entity in the input document.\n",
    "        # The API currently supports proper noun mentions.\n",
    "        for mention in entity.mentions:\n",
    "            print(u\"Mention text: {}\".format(mention.text.content))\n",
    "            # Get the mention type, e.g. PROPER for proper noun\n",
    "            print(\n",
    "                u\"Mention type: {}\".format(enums.EntityMention.Type(mention.type).name)\n",
    "            )\n",
    "\n",
    "    # Get the language of the text, which will be the same as\n",
    "    # the language specified in the request or, if not specified,\n",
    "    # the automatically-detected language.\n",
    "    print(u\"Language of the text: {}\".format(response.language))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_analyze_sentiment(text_content):\n",
    "    keyDIR = \"/Users/kunal/Documents/ResumeNLPVdart/APIKEYSGOOGLE/resumeMatcher-NLP_create_data.json\"\n",
    "    credentials = service_account.Credentials.from_service_account_file(keyDIR)\n",
    "    client = language_v1.LanguageServiceClient(credentials=credentials)\n",
    "    type_ = enums.Document.Type.PLAIN_TEXT\n",
    "    language = \"en\"\n",
    "    document = {\"content\": text_content, \"type\": type_, \"language\": language}\n",
    "    encoding_type = enums.EncodingType.UTF8\n",
    "    response = client.analyze_sentiment(document, encoding_type=encoding_type)\n",
    "    sentneceLISTAUTO = []\n",
    "    for sentence in response.sentences:\n",
    "        #print(u\"Sentence text: {}\".format(sentence.text.content))\n",
    "        #print(u\"Sentence sentiment score: {}\".format(sentence.sentiment.score))\n",
    "        #print(u\"Sentence sentiment magnitude: {}\".format(sentence.sentiment.magnitude))\n",
    "        sentneceLISTAUTO.append(sentence.text.content)\n",
    "    # Get the language of the text, which will be the same as\n",
    "    # the language specified in the request or, if not specified,\n",
    "    # the automatically-detected language.\n",
    "    #print(u\"Language of the text: {}\".format(response.language))\n",
    "    return sentneceLISTAUTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_analyze_sentiment(text_content):\n",
    "    keyDIR = \"/Users/kunal/Documents/ResumeNLPVdart/APIKEYSGOOGLE/resumeMatcher-NLP_create_data.json\"\n",
    "    credentials = service_account.Credentials.from_service_account_file(keyDIR)\n",
    "    client = language_v1.LanguageServiceClient(credentials=credentials)\n",
    "    type_ = enums.Document.Type.PLAIN_TEXT\n",
    "    language = \"en\"\n",
    "    document = {\"content\": text_content, \"type\": type_, \"language\": language}\n",
    "    encoding_type = enums.EncodingType.UTF8\n",
    "\n",
    "    response = client.analyze_sentiment(document, encoding_type=encoding_type)\n",
    "    # Get overall sentiment of the input document\n",
    "    print(u\"Document sentiment score: {}\".format(response.document_sentiment.score))\n",
    "    print(\n",
    "        u\"Document sentiment magnitude: {}\".format(\n",
    "            response.document_sentiment.magnitude\n",
    "        )\n",
    "    )\n",
    "    # Get sentiment for all sentences in the document\n",
    "    for sentence in response.sentences:\n",
    "        print(u\"Sentence text: {}\".format(sentence.text.content))\n",
    "        print(u\"Sentence sentiment score: {}\".format(sentence.sentiment.score))\n",
    "        print(u\"Sentence sentiment magnitude: {}\".format(sentence.sentiment.magnitude))\n",
    "\n",
    "    # Get the language of the text, which will be the same as\n",
    "    # the language specified in the request or, if not specified,\n",
    "    # the automatically-detected language.\n",
    "    print(u\"Language of the text: {}\".format(response.language))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "textSample = \"NISARGA HASSAN SREEDHAR\\nSan Jose, California |+1 (925) 789-8911| nisarga.nishu20@gmail.com | www.linkedin.com/in/nisarga-sreedhar-39938516b\\nEDUCATION:\\nMaster's in Electrical Engineering (Computer Networking), San Jose State University, California, USA.\\nCoursework: Internetworking, Broadband communications, Network Security, Internet of Things (IoT), Voice over IP\\nBachelor of Engineering in Telecommunication Engineering, Dayananda Sagar College of Engineering, Visvesvaraya\\nTechnological University, India\\nMay 2020\\nJune 2017\\nTECHNICAL SKILLS:\\nNetwork technologies: HTTP, DNS, DHCP, HTTPS, TLS-SSL, TCP/IP, UDP, IPV4, IPV6, ICMP, OSPF, BGP, ARP, VLAN, STP,\\nSIP, IPS, IDS, NAT, IS-IS, 802.11, MPLS, WPA2, WPA3, Packet level troubleshooting\\nProgramming: Python\\nOS Platform: Linux (Ubuntu, CentOS), Kali Linux, Cisco IOS\\nTools and IDE: Advanced Design System (ADS), Wireshark, VMware Workstation, VirtualBox, GNS3, Cisco Packet Tracer, PUTTY\\nCERTIFICATION:\\nCisco Certified Network Associate (CCNA) 200-301\\nAWS Certified Cloud Practitioner (CLF-C01)\\n(In Progress)\\n(In progress)\\nEXPERIENCE:\\nJune 2019 - July 2019\\nMarmon Food & Beverages Technologies, Cornelius, India\\nNetwork Engineer Intern\\nPython based Serial Communication (IoT)\\nUsed an Iot Dongle to read a file, convert it into a packet by adding header and footer and transmit serially.\\nPython code was written to send the file from dongle to Food Holding Bin.\\nACADEMIC PROJECTS:\\nSecure routing in IoT networks\\nAug 2019 - current\\nDesign and configure an IoT based network using Cisco Packet Tracer.\\nPerform a Man in the Middle attack to one of the devices using Kali Linux.\\nDetection of the attack and solution to the problem faced.\\nIllumino: IoT Smart Light\\nAug 2019 - Dec 2019\\nCreate a hardware of an IoT smart light using Arduino ESP8266 and Cayenne IoT Platform.\\nDesigned to operate in three modes: Auto mode, Lamp mode, Security mode.\\nUse of Cayenne web application to detect temperature and provides a siren at thresholds.\\nVoice over IP for Wireless Ad Hoc Networks (WANET)\\nAug 2019 - Dec 2019\\nSimple Call Establishment between two clients in a WANET that have registered with the Asterisk server.\\nCall on Hold with one user client to attend another client.\\nCall Conferencing between all three clients, all performed using X-Lite softphone software.\\nExperiencing Virtualization using Virtual Box\\nJan 2019 - April 2019\\nWorked on Open vSwitch in Virtual Box on an Ubuntu machine to run ovs and its versions successfully.\\nDemonstrated how the VLANS are implemented, three VMs and one virtual switch is created.\\nAttempted to communicate between the VMs and observed the PING result.\\nCorporate Company Network Design\\nAug 2018 - Dec 2018\\n• Designed and implemented a basic corporate network topology for the interconnection between offices with\\nswitches, routers, and hosts.\\n• Implemented the design using routing protocols such as OSPF, BGP, DNS, VLAN, STP, IP, DHCP and HSRP.\\nTested and troubleshot configurations in the console to check the communication between the networks.\\nDesign of X-Band 8PSK Modulator using ADS\\nJan 2017 - April 2017\\nDesigned various components of the modulator used in a satellite at ISRO (Indian Space Research Organization), Bangalore.\\nPerformed optimization of the components at 8.75GHZ frequency using the tools available in ADS to obtain the desired results of\\nInsertion loss, Return loss and Isolation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document sentiment score: 0.0\n",
      "Document sentiment magnitude: 1.899999976158142\n",
      "Sentence text: NISARGA HASSAN SREEDHAR\n",
      "San Jose, California |+1 (925) 789-8911| nisarga.nishu20@gmail.com | www.linkedin.com/in/nisarga-sreedhar-39938516b\n",
      "EDUCATION:\n",
      "Master's in Electrical Engineering (Computer Networking), San Jose State University, California, USA.\n",
      "Sentence sentiment score: 0.0\n",
      "Sentence sentiment magnitude: 0.0\n",
      "Sentence text: Coursework: Internetworking, Broadband communications, Network Security, Internet of Things (IoT), Voice over IP\n",
      "Bachelor of Engineering in Telecommunication Engineering, Dayananda Sagar College of Engineering, Visvesvaraya\n",
      "Technological University, India\n",
      "May 2020\n",
      "June 2017\n",
      "TECHNICAL SKILLS:\n",
      "Network technologies: HTTP, DNS, DHCP, HTTPS, TLS-SSL, TCP/IP, UDP, IPV4, IPV6, ICMP, OSPF, BGP, ARP, VLAN, STP,\n",
      "SIP, IPS, IDS, NAT, IS-IS, 802.11, MPLS, WPA2, WPA3, Packet level troubleshooting\n",
      "Programming: Python\n",
      "OS Platform: Linux (Ubuntu, CentOS), Kali Linux, Cisco IOS\n",
      "Tools and IDE: Advanced Design System (ADS), Wireshark, VMware Workstation, VirtualBox, GNS3, Cisco Packet Tracer, PUTTY\n",
      "CERTIFICATION:\n",
      "Cisco Certified Network Associate (CCNA) 200-301\n",
      "AWS Certified Cloud Practitioner (CLF-C01)\n",
      "(In Progress)\n",
      "(In progress)\n",
      "EXPERIENCE:\n",
      "June 2019 - July 2019\n",
      "Marmon Food & Beverages Technologies, Cornelius, India\n",
      "Network Engineer Intern\n",
      "Python based Serial Communication (IoT)\n",
      "Used an Iot Dongle to read a file, convert it into a packet by adding header and footer and transmit serially.\n",
      "Sentence sentiment score: 0.10000000149011612\n",
      "Sentence sentiment magnitude: 0.10000000149011612\n",
      "Sentence text: Python code was written to send the file from dongle to Food Holding Bin.\n",
      "Sentence sentiment score: 0.0\n",
      "Sentence sentiment magnitude: 0.0\n",
      "Sentence text: ACADEMIC PROJECTS:\n",
      "Secure routing in IoT networks\n",
      "Aug 2019 - current\n",
      "Design and configure an IoT based network using Cisco Packet Tracer.\n",
      "Sentence sentiment score: 0.0\n",
      "Sentence sentiment magnitude: 0.0\n",
      "Sentence text: Perform a Man in the Middle attack to one of the devices using Kali Linux.\n",
      "Sentence sentiment score: -0.10000000149011612\n",
      "Sentence sentiment magnitude: 0.10000000149011612\n",
      "Sentence text: Detection of the attack and solution to the problem faced.\n",
      "Sentence sentiment score: 0.0\n",
      "Sentence sentiment magnitude: 0.0\n",
      "Sentence text: Illumino: IoT Smart Light\n",
      "Aug 2019 - Dec 2019\n",
      "Create a hardware of an IoT smart light using Arduino ESP8266 and Cayenne IoT Platform.\n",
      "Sentence sentiment score: 0.0\n",
      "Sentence sentiment magnitude: 0.0\n",
      "Sentence text: Designed to operate in three modes: Auto mode, Lamp mode, Security mode.\n",
      "Sentence sentiment score: 0.0\n",
      "Sentence sentiment magnitude: 0.0\n",
      "Sentence text: Use of Cayenne web application to detect temperature and provides a siren at thresholds.\n",
      "Sentence sentiment score: 0.10000000149011612\n",
      "Sentence sentiment magnitude: 0.10000000149011612\n",
      "Sentence text: Voice over IP for Wireless Ad Hoc Networks (WANET)\n",
      "Aug 2019 - Dec 2019\n",
      "Simple Call Establishment between two clients in a WANET that have registered with the Asterisk server.\n",
      "Sentence sentiment score: -0.10000000149011612\n",
      "Sentence sentiment magnitude: 0.10000000149011612\n",
      "Sentence text: Call on Hold with one user client to attend another client.\n",
      "Sentence sentiment score: -0.30000001192092896\n",
      "Sentence sentiment magnitude: 0.30000001192092896\n",
      "Sentence text: Call Conferencing between all three clients, all performed using X-Lite softphone software.\n",
      "Sentence sentiment score: 0.0\n",
      "Sentence sentiment magnitude: 0.0\n",
      "Sentence text: Experiencing Virtualization using Virtual Box\n",
      "Jan 2019 - April 2019\n",
      "Worked on Open vSwitch in Virtual Box on an Ubuntu machine to run ovs and its versions successfully.\n",
      "Sentence sentiment score: 0.10000000149011612\n",
      "Sentence sentiment magnitude: 0.10000000149011612\n",
      "Sentence text: Demonstrated how the VLANS are implemented, three VMs and one virtual switch is created.\n",
      "Sentence sentiment score: 0.0\n",
      "Sentence sentiment magnitude: 0.0\n",
      "Sentence text: Attempted to communicate between the VMs and observed the PING result.\n",
      "Sentence sentiment score: 0.0\n",
      "Sentence sentiment magnitude: 0.0\n",
      "Sentence text: Corporate Company Network Design\n",
      "Aug 2018 - Dec 2018\n",
      "Sentence sentiment score: 0.0\n",
      "Sentence sentiment magnitude: 0.0\n",
      "Sentence text: Designed and implemented a basic corporate network topology for the interconnection between offices with\n",
      "switches, routers, and hosts.\n",
      "Sentence sentiment score: 0.0\n",
      "Sentence sentiment magnitude: 0.0\n",
      "Sentence text: Implemented the design using routing protocols such as OSPF, BGP, DNS, VLAN, STP, IP, DHCP and HSRP.\n",
      "Sentence sentiment score: 0.0\n",
      "Sentence sentiment magnitude: 0.0\n",
      "Sentence text: Tested and troubleshot configurations in the console to check the communication between the networks.\n",
      "Sentence sentiment score: 0.0\n",
      "Sentence sentiment magnitude: 0.0\n",
      "Sentence text: Design of X-Band 8PSK Modulator using ADS\n",
      "Jan 2017 - April 2017\n",
      "Designed various components of the modulator used in a satellite at ISRO (Indian Space Research Organization), Bangalore.\n",
      "Sentence sentiment score: -0.10000000149011612\n",
      "Sentence sentiment magnitude: 0.10000000149011612\n",
      "Sentence text: Performed optimization of the components at 8.75GHZ frequency using the tools available in ADS to obtain the desired results of\n",
      "Insertion loss, Return loss and Isolation\n",
      "Sentence sentiment score: -0.10000000149011612\n",
      "Sentence sentiment magnitude: 0.10000000149011612\n",
      "Language of the text: en\n"
     ]
    }
   ],
   "source": [
    "sample_analyze_sentiment(textSample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import language\n",
    "from google.cloud.language import enums\n",
    "from google.cloud.language import types\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import language_v1\n",
    "from google.cloud.language_v1 import enums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractEntities(text_content):\n",
    "    keyDIR = \"/Users/kunal/Documents/ResumeNLPVdart/APIKEYSGOOGLE/resumeMatcher-NLP_create_data.json\"\n",
    "    credentials = service_account.Credentials.from_service_account_file(keyDIR)\n",
    "    client = language_v1.LanguageServiceClient(credentials=credentials)\n",
    "    type_ = enums.Document.Type.PLAIN_TEXT\n",
    "    language = \"en\"\n",
    "    document = {\"content\": text_content, \"type\": type_, \"language\": language}\n",
    "    encoding_type = enums.EncodingType.UTF8\n",
    "    response = client.analyze_entities(document, encoding_type=encoding_type)\n",
    "    nonuseEntities = [\"OTHER\", \"NUMBER\"]\n",
    "    allEntitiesExtracted = []\n",
    "    for entity in response.entities:\n",
    "        currentRunningEntity = []\n",
    "        #print(entity.name)\n",
    "        #print(enums.Entity.Type(entity.type).name)\n",
    "        if enums.Entity.Type(entity.type).name not in nonuseEntities:\n",
    "            print(\"Detected \" + entity.name + \" as \" + enums.Entity.Type(entity.type).name)\n",
    "            currentRunningEntity.append(entity.name)\n",
    "            currentRunningEntity.append(enums.Entity.Type(entity.type).name)\n",
    "            allEntitiesExtracted.append(currentRunningEntity)\n",
    "        else: \n",
    "            for mention in entity.mentions:\n",
    "                if enums.EntityMention.Type(mention.type).name == \"PROPER\":\n",
    "                    print(\"Detected \" + mention.text.content + \" as \" + enums.EntityMention.Type(mention.type).name)\n",
    "                    currentRunningEntity.append(mention.text.content)\n",
    "                    currentRunningEntity.append(enums.EntityMention.Type(mention.type).name)\n",
    "                    allEntitiesExtracted.append(currentRunningEntity)\n",
    "        #allEntitiesExtracted.append(currentRunningEntity) if currentRunningEntity != [] else print(\"-\")\n",
    "        #allEntitiesExtracted.append(currentRunningEntity)   \n",
    "    finalArray = []\n",
    "    runningEnitityCount = 0\n",
    "    positionValuesEnitiesList = []\n",
    "    for i in range(len(allEntitiesExtracted)):\n",
    "        arrayrun = []\n",
    "        arrayrun.append(allEntitiesExtracted[i][0])\n",
    "        arrayrun.append(allEntitiesExtracted[i][1])\n",
    "        arrayrun.append(text_content.find(allEntitiesExtracted[i][0]))\n",
    "        positionValuesEnitiesList.append(arrayrun)\n",
    "    positionValuesEnitiesList = sorted(positionValuesEnitiesList, key=lambda x: x[2])\n",
    "    #print(positionValuesEnitiesList)\n",
    "    runningPositionValuesCount = 0\n",
    "    for numb in range(len(positionValuesEnitiesList)-1):\n",
    "        distancebetween = 0\n",
    "        lenWord = len(positionValuesEnitiesList[numb][0])\n",
    "        wordPosition = positionValuesEnitiesList[numb][2]\n",
    "        distancebetween =((positionValuesEnitiesList[numb+1][2] - (wordPosition + lenWord)))\n",
    "        sameElement = True if positionValuesEnitiesList[numb][1] == positionValuesEnitiesList[numb+1][1] else False\n",
    "        #print(sameElement)\n",
    "        if distancebetween < 2.5 and sameElement:\n",
    "            #print(positionValuesEnitiesList[numb])\n",
    "            returnArray = [text_content[positionValuesEnitiesList[numb][2]:positionValuesEnitiesList[numb+1][2]+len(positionValuesEnitiesList[numb+1][1])]]\n",
    "            returnArray.append(positionValuesEnitiesList[numb][1])\n",
    "            finalArray.append(returnArray)\n",
    "        else: \n",
    "            finalArray.append([positionValuesEnitiesList[numb][0], positionValuesEnitiesList[numb][1]])\n",
    "    #print(distancebetween)\n",
    "    #if distancebetween/len(positionValuesEnitiesList)-1 < 2.5:\n",
    "        #returnArray = [text_content[positionValuesEnitiesList[0][2]:positionValuesEnitiesList[-1][2]+len(positionValuesEnitiesList[-1][1])]]\n",
    "        #returnArray.append(positionValuesEnitiesList[0][1])\n",
    "        #print(returnArray)\n",
    "    return finalArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_classify_text(text_content):\n",
    "    keyDIR = \"/Users/kunal/Documents/ResumeNLPVdart/APIKEYSGOOGLE/resumeMatcher-NLP_create_data.json\"\n",
    "    credentials = service_account.Credentials.from_service_account_file(keyDIR)\n",
    "    client = language_v1.LanguageServiceClient(credentials=credentials)\n",
    "    type_ = enums.Document.Type.PLAIN_TEXT\n",
    "    language = \"en\"\n",
    "    document = {\"content\": text_content, \"type\": type_, \"language\": language}\n",
    "    response = client.classify_text(document)\n",
    "    # Loop through classified categories returned from the API\n",
    "    categoryList = []\n",
    "    for category in response.categories:\n",
    "        # Get the name of the category representing the document.\n",
    "        # See the predefined taxonomy of categories:\n",
    "        # https://cloud.google.com/natural-language/docs/categories\n",
    "        # print(u\"Category name: {}\".format(category.name))\n",
    "        categoryList.append([category.name, category.confidence])\n",
    "    return categoryList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "extractEntities() missing 1 required positional argument: 'text_content'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-268-3a44f7218c90>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mextractEntities\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: extractEntities() missing 1 required positional argument: 'text_content'"
     ]
    }
   ],
   "source": [
    "entityArray, importantEntitiesArray = extractEntities(textSample, 0.05)\n",
    "allPossibleCategory = sample_classify_text(textSample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected IoT as ORGANIZATION\n",
      "Detected DNS as PROPER\n",
      "Detected Advanced Design System as PROPER\n",
      "Detected California as LOCATION\n",
      "Detected San Jose State University as ORGANIZATION\n",
      "Detected San Jose as LOCATION\n",
      "Detected NISARGA as PERSON\n",
      "Detected HASSAN SREEDHAR as PERSON\n",
      "Detected USA as LOCATION\n",
      "Detected Master's in Electrical Engineering as WORK_OF_ART\n",
      "Detected Telecommunication Engineering as PROPER\n",
      "Detected Voice over IP as PROPER\n",
      "Detected VLAN as PROPER\n",
      "Detected VLAN as PROPER\n",
      "Detected VLANS as PROPER\n",
      "Detected Visvesvaraya Technological University as ORGANIZATION\n",
      "Detected BGP as PROPER\n",
      "Detected BGP as PROPER\n",
      "Detected OSPF as PROPER\n",
      "Detected OSPF as PROPER\n",
      "Detected STP as ORGANIZATION\n",
      "Detected DHCP as PROPER\n",
      "Detected DHCP as PROPER\n",
      "Detected India as LOCATION\n",
      "Detected Dayananda Sagar College of Engineering as ORGANIZATION\n",
      "Detected IoT as ORGANIZATION\n",
      "Detected IoT as ORGANIZATION\n",
      "Detected Serial Communication as WORK_OF_ART\n",
      "Detected ADS as WORK_OF_ART\n",
      "Detected Network Engineer Intern Python as ORGANIZATION\n",
      "Detected Kali Linux as PROPER\n",
      "Detected Kali Linux as PROPER\n",
      "Detected attack as EVENT\n",
      "Detected Linux as CONSUMER_GOOD\n",
      "Detected Python as ORGANIZATION\n",
      "Detected attack as EVENT\n",
      "Detected Cisco Packet Tracer as CONSUMER_GOOD\n",
      "Detected footer as PERSON\n",
      "Detected devices as CONSUMER_GOOD\n",
      "Detected HTTP as PROPER\n",
      "Detected HTTPS as PROPER\n",
      "Detected Ubuntu as ORGANIZATION\n",
      "Detected IS-IS as PROPER\n",
      "Detected Wireless Ad Hoc Networks as PROPER\n",
      "Detected web application as CONSUMER_GOOD\n",
      "Detected ICMP as ORGANIZATION\n",
      "Detected CentOS as CONSUMER_GOOD\n",
      "Detected UDP as ORGANIZATION\n",
      "Detected TLS-SSL as PROPER\n",
      "Detected IPV6 as PROPER\n",
      "Detected NAT as PERSON\n",
      "Detected IPS as ORGANIZATION\n",
      "Detected SIP as ORGANIZATION\n",
      "Detected MPLS as PROPER\n",
      "Detected ARP as ORGANIZATION\n",
      "Detected IDS as ORGANIZATION\n",
      "Detected WPA2 as PROPER\n",
      "Detected IPV4 as PROPER\n",
      "Detected WPA3 as PROPER\n",
      "Detected routers as CONSUMER_GOOD\n",
      "Detected client as PERSON\n",
      "Detected PUTTY\n",
      "CERTIFICATION as PROPER\n",
      "Detected Marmon Food & Beverages Technologies as CONSUMER_GOOD\n",
      "Detected Iot Dongle as PROPER\n",
      "Detected Cayenne IoT Platform as CONSUMER_GOOD\n",
      "Detected hosts as PERSON\n",
      "Detected offices as LOCATION\n",
      "Detected Wireshark as PROPER\n",
      "Detected Cisco IOS Tools as CONSUMER_GOOD\n",
      "Detected VirtualBox as CONSUMER_GOOD\n",
      "Detected Cisco Certified Network Associate as PROPER\n",
      "Detected CCNA as PROPER\n",
      "Detected IoT Smart Light as PROPER\n",
      "Detected Food Holding Bin as PERSON\n",
      "Detected Man in the Middle as WORK_OF_ART\n",
      "Detected CLF-C01 as PROPER\n",
      "Detected GNS3 as PROPER\n",
      "Detected Cornelius as PERSON\n",
      "Detected VMware Workstation as CONSUMER_GOOD\n",
      "Detected Certified Cloud Practitioner as PERSON\n",
      "Detected Corporate Company Network Design as PROPER\n",
      "Detected Arduino ESP8266 as PROPER\n",
      "Detected Illumino as PERSON\n",
      "Detected Lamp as PROPER\n",
      "Detected console as CONSUMER_GOOD\n",
      "Detected satellite as LOCATION\n",
      "Detected VMs as CONSUMER_GOOD\n",
      "Detected PING as ORGANIZATION\n",
      "Detected clients as PERSON\n",
      "Detected user client as PERSON\n",
      "Detected clients as PERSON\n",
      "Detected softphone software as CONSUMER_GOOD\n",
      "Detected Conferencing as EVENT\n",
      "Detected X-Band 8PSK Modulator as PROPER\n",
      "Detected versions as CONSUMER_GOOD\n",
      "Detected machine as CONSUMER_GOOD\n",
      "Detected HSRP as PROPER\n",
      "Detected ISRO as ORGANIZATION\n",
      "Detected Bangalore as LOCATION\n",
      "Detected WANET as ORGANIZATION\n",
      "Detected Ubuntu as ORGANIZATION\n",
      "Detected Asterisk as PROPER\n",
      "Detected Open vSwitch as PROPER\n",
      "Detected +1 (925) 789-8911 as PHONE_NUMBER\n",
      "Detected May 2020 as DATE\n",
      "Detected June 2017 as DATE\n",
      "Detected June 2019 as DATE\n",
      "Detected July 2019 as DATE\n",
      "Detected Aug 2019 as DATE\n",
      "Detected Aug 2019 as DATE\n",
      "Detected Dec 2019 as DATE\n",
      "Detected Aug 2019 as DATE\n",
      "Detected Dec 2019 as DATE\n",
      "Detected Jan 2019 as DATE\n",
      "Detected April 2019 as DATE\n",
      "Detected Aug 2018 as DATE\n",
      "Detected Dec 2018 as DATE\n",
      "Detected Jan 2017 as DATE\n",
      "Detected April 2017 as DATE\n",
      "Detected ESP8266 as PRICE\n"
     ]
    }
   ],
   "source": [
    "entityArray = extractEntities(textSample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['', 'ORGANIZATION'],\n",
       " ['Network Engineer Intern Python', 'ORGANIZATION'],\n",
       " ['Cisco IOS Tools', 'CONSUMER_GOOD'],\n",
       " ['NISARGA HASSAN', 'PERSON'],\n",
       " ['HASSAN SREEDHAR', 'PERSON'],\n",
       " ['San Jose, Californ', 'LOCATION'],\n",
       " ['California', 'LOCATION'],\n",
       " ['+1 (925) 789-8911', 'PHONE_NUMBER'],\n",
       " [\"Master's in Electrical Engineering\", 'WORK_OF_ART'],\n",
       " ['San Jose State University', 'ORGANIZATION'],\n",
       " ['USA', 'LOCATION'],\n",
       " ['IoT), Voice ', 'ORGANIZATION'],\n",
       " ['IoT), Voice ', 'ORGANIZATION'],\n",
       " ['IoT', 'ORGANIZATION'],\n",
       " ['Voice over IP', 'PROPER'],\n",
       " ['Telecommunication Engineering', 'PROPER'],\n",
       " ['Dayananda Sagar College of Engineering', 'ORGANIZATION'],\n",
       " ['India', 'LOCATION'],\n",
       " ['May 2020\\nJune', 'DATE'],\n",
       " ['June 2017', 'DATE'],\n",
       " ['HTTP, DNS, D', 'PROPER'],\n",
       " ['DNS, DHCP, ', 'PROPER'],\n",
       " ['DHCP, ', 'PROPER'],\n",
       " ['DHCP, HTTPS,', 'PROPER'],\n",
       " ['HTTPS, TLS-SS', 'PROPER'],\n",
       " ['TLS-SSL', 'PROPER'],\n",
       " ['UDP', 'ORGANIZATION'],\n",
       " ['IPV4, IPV6, ', 'PROPER'],\n",
       " ['IPV6', 'PROPER'],\n",
       " ['ICMP', 'ORGANIZATION'],\n",
       " ['OSPF, ', 'PROPER'],\n",
       " ['OSPF, BGP, A', 'PROPER'],\n",
       " ['BGP, A', 'PROPER'],\n",
       " ['BGP', 'PROPER'],\n",
       " ['ARP', 'ORGANIZATION'],\n",
       " ['VLAN, ', 'PROPER'],\n",
       " ['VLAN, ', 'PROPER'],\n",
       " ['VLAN', 'PROPER'],\n",
       " ['STP,\\nSIP, IPS, ID', 'ORGANIZATION'],\n",
       " ['SIP, IPS, IDS, NA', 'ORGANIZATION'],\n",
       " ['IPS, IDS, NAT, IS', 'ORGANIZATION'],\n",
       " ['IDS', 'ORGANIZATION'],\n",
       " ['NAT', 'PERSON'],\n",
       " ['IS-IS', 'PROPER'],\n",
       " ['MPLS, WPA2, ', 'PROPER'],\n",
       " ['WPA2, WPA3, ', 'PROPER'],\n",
       " ['WPA3', 'PROPER'],\n",
       " ['Python', 'ORGANIZATION'],\n",
       " ['Linux', 'CONSUMER_GOOD'],\n",
       " ['Ubuntu, Cent', 'ORGANIZATION'],\n",
       " ['Ubuntu', 'ORGANIZATION'],\n",
       " ['CentOS', 'CONSUMER_GOOD'],\n",
       " ['Kali L', 'PROPER'],\n",
       " ['Kali Linux', 'PROPER'],\n",
       " ['Advanced Design System', 'PROPER'],\n",
       " ['ADS', 'WORK_OF_ART'],\n",
       " ['Wireshark', 'PROPER'],\n",
       " ['VMware Workstation, VirtualBox, G', 'CONSUMER_GOOD'],\n",
       " ['VirtualBox', 'CONSUMER_GOOD'],\n",
       " ['GNS3', 'PROPER'],\n",
       " ['Cisco Packet Tracer', 'CONSUMER_GOOD'],\n",
       " ['PUTTY\\nCERTIFICATION:\\nCisco ', 'PROPER'],\n",
       " ['Cisco ', 'PROPER'],\n",
       " ['Cisco Certified Network Associate', 'PROPER'],\n",
       " ['Certified Cloud Practitioner', 'PERSON'],\n",
       " ['CLF-C01', 'PROPER'],\n",
       " ['June 2019', 'DATE'],\n",
       " ['July 2019', 'DATE'],\n",
       " ['Marmon Food & Beverages Technologies', 'CONSUMER_GOOD'],\n",
       " ['Cornelius', 'PERSON'],\n",
       " ['Serial Communication', 'WORK_OF_ART'],\n",
       " ['Iot Dongle', 'PROPER'],\n",
       " ['footer', 'PERSON'],\n",
       " ['Food Holding Bin', 'PERSON'],\n",
       " ['Aug ', 'DATE'],\n",
       " ['Aug ', 'DATE'],\n",
       " ['Aug 2019', 'DATE'],\n",
       " ['Man in the Middle', 'WORK_OF_ART'],\n",
       " ['attac', 'EVENT'],\n",
       " ['attack', 'EVENT'],\n",
       " ['devices', 'CONSUMER_GOOD'],\n",
       " ['Illumino', 'PERSON'],\n",
       " ['IoT Smart Light', 'PROPER'],\n",
       " ['Dec ', 'DATE'],\n",
       " ['Dec 2019', 'DATE'],\n",
       " ['Arduino ESP8266', 'PROPER'],\n",
       " ['ESP8266', 'PRICE'],\n",
       " ['Cayenne IoT Platform', 'CONSUMER_GOOD'],\n",
       " ['Lamp', 'PROPER'],\n",
       " ['web application', 'CONSUMER_GOOD'],\n",
       " ['Wireless Ad Hoc Networks', 'PROPER'],\n",
       " ['WANET', 'ORGANIZATION'],\n",
       " ['client', 'PERSON'],\n",
       " ['client', 'PERSON'],\n",
       " ['clients', 'PERSON'],\n",
       " ['Asterisk', 'PROPER'],\n",
       " ['user client', 'PERSON'],\n",
       " ['Conferencing', 'EVENT'],\n",
       " ['softphone software', 'CONSUMER_GOOD'],\n",
       " ['Jan 2019', 'DATE'],\n",
       " ['April 2019', 'DATE'],\n",
       " ['Open vSwitch', 'PROPER'],\n",
       " ['machine', 'CONSUMER_GOOD'],\n",
       " ['versions', 'CONSUMER_GOOD'],\n",
       " ['VMs', 'CONSUMER_GOOD'],\n",
       " ['PING', 'ORGANIZATION'],\n",
       " ['Corporate Company Network Design', 'PROPER'],\n",
       " ['Aug 2018', 'DATE'],\n",
       " ['Dec 2018', 'DATE'],\n",
       " ['offices', 'LOCATION'],\n",
       " ['routers', 'CONSUMER_GOOD'],\n",
       " ['hosts', 'PERSON'],\n",
       " ['HSRP', 'PROPER'],\n",
       " ['console', 'CONSUMER_GOOD'],\n",
       " ['X-Band 8PSK Modulator', 'PROPER'],\n",
       " ['Jan 2017', 'DATE'],\n",
       " ['April 2017', 'DATE'],\n",
       " ['satellite', 'LOCATION'],\n",
       " ['ISRO', 'ORGANIZATION']]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entityArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
